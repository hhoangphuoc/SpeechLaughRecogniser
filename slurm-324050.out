	Adding nVidia Cuda Toolkit 11.7
	Adding nVidia cuDNN 8.6 (.0.163)
	Adding nVidia TensorRT 8.6 (.0.12)
	Adding nvtop
Date              = Tue Nov  5 03:40:26 PM CET 2024
Hostname          = ctit086
Working Directory = /home/s2587130/SpeechLaughRecogniser
Name of nodes used          : ctit086
Gpu devices                 : 3
Starting worker: 
2024-11-05 16:05:44.876881: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-05 16:07:25.704166: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-05 16:07:55.679458: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-05 16:07:57.563253: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-05 16:09:17.176847: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-05 16:20:26.546074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Evaluate Whisper Model - openai/whisper-small 

Loaded Token Dataset: 
Dataset({
    features: ['audio', 'sampling_rate', 'transcript'],
    num_rows: 255826
})
Loaded Word Dataset: 
Dataset({
    features: ['audio', 'sampling_rate', 'transcript'],
    num_rows: 255824
})
Filtered Laughter Dataset: 
Dataset({
    features: ['audio', 'sampling_rate', 'transcript'],
    num_rows: 7536
})
Example of Laughter Dataset: 
{'audio': {'path': 'sw02325A_25763475_270175625.wav', 'array': array([-0.00073242, -0.00454712,  0.        , ..., -0.07614136,
       -0.06033325, -0.03027344]), 'sampling_rate': 16000}, 'sampling_rate': 16000, 'transcript': 'uh you know they were [SPEECH_LAUGH] when i would get home now i understand kids go out and play and they get [SPEECH_LAUGH] but i mean filthy i am talking sand in the [SPEECH_LAUGH] and the [SPEECH_LAUGH] and the [SPEECH_LAUGH] and the and i was like gosh and then'}
Filtered Laughing Words Dataset: 
Dataset({
    features: ['audio', 'sampling_rate', 'transcript'],
    num_rows: 7534
})
Example of Laughing Words Dataset: 
{'audio': {'path': 'sw02325A_25763475_270175625.wav', 'array': array([-0.00073242, -0.00454712,  0.        , ..., -0.07614136,
       -0.06033325, -0.03027344]), 'sampling_rate': 16000}, 'sampling_rate': 16000, 'transcript': 'uh you know they were FILTHY when i would get home now i understand kids go out and play and they get DIRTY but i mean filthy i am talking sand in the EARS and the EYES and the HAIR and the and i was like gosh and then'}
Total runtime: 9115.948365688324 seconds
